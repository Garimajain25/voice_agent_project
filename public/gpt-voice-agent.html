<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>GPT Voice Agent (Text-to-Speech Version)</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
        background: linear-gradient(135deg,rgb(3, 80, 16) 0%,rgb(202, 235, 190) 100%);
        min-height: 100vh;
        display: flex;
        align-items: center;
        justify-content: center;
        padding: 20px;
      }
      
      .container {
        background: white;
        border-radius: 20px;
        box-shadow: 0 20px 40px rgba(0,0,0,0.1);
        padding: 40px;
        max-width: 800px; 
        width: 100%;
        text-align: center;
      }
      
      h1 {
        color: #155724;
        margin-bottom: 10px;
        font-size: 3rem;
        font-weight: 700;
      }
      
      .subtitle {
        color: #155724;
        margin-bottom: 30px;
        font-size: 1.1rem;
      }
      
      .status-container {
        margin: 20px 0;
        padding: 15px;
        border-radius: 10px;
        background: #d4edda;
        border: 2px solid #c3e6cb;
        transition: all 0.3s ease;
        color: #155724;
      }
      
      .status-container.connected {
        background: #d4edda;
        border-color: #c3e6cb;
        color: #155724;
      }
      
      .status-container.error {
        background: #f8d7da;
        border-color: #f5c6cb;
        color: #721c24;
      }
      
      .status-container.connecting {
        background: #fff3cd;
        border-color: #ffeaa7;
        color: #856404;
      }
      
      #status {
        font-weight: 600;
        font-size: 1.1rem;
      }
      
      .controls {
        margin: 30px 0;
        text-align: left;
      }
      
      .control-group {
        margin-bottom: 20px;
      }
      
      label {
        display: block;
        margin-bottom: 8px;
        font-weight: 600;
        color: #155724;
        font-weight: bold;
      }
      
      input, select, textarea {
        width: 100%;
        padding: 12px;
        border: 2px solid #155724;
        border-radius: 8px;
        font-size: 16px;
        transition: border-color 0.3s ease;
        color: #155724;
      }
      
      input:focus, select:focus, textarea:focus {
        outline: none;
        border-color: #667eea;
      }
      
      textarea {
        resize: vertical;
        min-height: 80px;
      }
      
      .btn {
        background: #d4edda;
        color: #155724;
        border: none;
        padding: 15px 30px;
        font-size: 18px;
        font-weight: bold;
        border-radius: 50px;
        cursor: pointer;
        transition: all 0.3s ease;
        width: 100%;
        margin: 10px 0;
      }
      
      .btn:hover {
        transform: translateY(-2px);
        box-shadow: 0 10px 20px rgba(102, 126, 234, 0.3);
      }
      
      .btn:disabled {
        background: #ccc;
        cursor: not-allowed;
        transform: none;
        box-shadow: none;
      }
      
      .btn.secondary {
        background: #155724;
        color: white;
      }
      
      .conversation {
        margin: 20px 0;
        max-height: 600px;  
        overflow-y: auto;
        overflow-x: hidden;
        border: 1px solid #c3e6cb;
        border-radius: 10px;
        padding: 15px;
        text-align: left;
        background: #f8f9fa;
        word-wrap: break-word;
      }
      
      .message {
        margin: 10px 0;
        padding: 10px;
        border-radius: 8px;
        overflow: hidden;
        word-wrap: break-word;
      }
      
      .message.user {
        background: linear-gradient(135deg, #d4edda 0%, #c3e6cb 100%);
        margin-left: 20px;
        color: #155724;
        font-weight: bold;
        border: 2px solid #28a745;
        box-shadow: 0 4px 8px rgba(40, 167, 69, 0.2);
      }
      
      .message.assistant {
        background: #155724;
        margin-right: 20px;
        color: white;
        box-shadow: 0 4px 8px rgba(40, 167, 69, 0.3);
      }
      
      .message-label {
        font-weight: 600;
        font-size: 12px;
        color: #155724;
        margin-bottom: 5px;
      }
      
      .message.assistant .message-label {
        color: white;
      }
      
      .generated-image {
        max-width: 100%;
        max-height: 300px;
        width: auto;
        height: auto;
        border-radius: 8px;
        margin: 10px 0;
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        cursor: pointer;
        transition: transform 0.3s ease;
        object-fit: contain;
        display: block;
        margin-left: auto;
        margin-right: auto;
      }
      
      .generated-image:hover {
        transform: scale(1.02);
      }
      
      .image-prompt {
        font-style: italic;
        color: white;
        font-size: 12px;
        margin-top: 5px;
      }
      
      .download-btn {
        background: #28a745;
        color: white;
        border: none;
        padding: 5px 10px;
        border-radius: 4px;
        font-size: 12px;
        cursor: pointer;
        margin-top: 5px;
        transition: background 0.3s ease;
      }
      
      .download-btn:hover {
        background: #218838;
      }
      
      .voice-options {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(120px, 1fr));
        gap: 10px;
        margin-top: 10px;
      }
      
      .voice-option {
        padding: 10px;
        border: 2px solid #e9ecef;
        border-radius: 8px;
        cursor: pointer;
        transition: all 0.3s ease;
        text-align: center;
        color: #155724;
      }
      
      .voice-option:hover {
        border-color: #667eea;
        background: #f8f9ff;
      }
      
      .voice-option.selected {
        border-color: #c3e6cb;
        background: #d4edda;
        color: #155724;
        font-weight: bold;
      }
      
      .instructions {
        margin-top: 20px;
        padding: 15px;
        background: #f8f9fa;
        border-radius: 10px;
        font-size: 14px;
        color: #155724;
        text-align: left;
      }
      
      .instructions h3 {
        margin-bottom: 10px;
        color: #155724;
      }
      
      .instructions ul {
        margin-left: 20px;
      }
      
      .instructions li {
        margin-bottom: 5px;
      }
      
      @media (max-width: 600px) {
        .container {
          padding: 20px;
          margin: 10px;
        }
        
        h1 {
          font-size: 2rem;
        }
      }
    </style>
  </head>
  <body>
    <div class="container">
      <h1>üé§ Voice Agent</h1>
      <p class="subtitle">Powered by GPT-4o + Text-to-Speech</p>
      
      <div class="status-container" id="statusContainer">
        <div id="status">Ready to chat</div>
      </div>
      
      <div id="edgeWarning" style="display: none; background: #fff3cd; border: 1px solid #ffeaa7; border-radius: 8px; padding: 15px; margin: 20px 0; color: #856404;">
        <strong>‚ö†Ô∏è Edge Browser Detected</strong><br>
        For best audio experience in Edge, please click anywhere on the page first to enable audio playback.
      </div>
      
      <div class="controls">
        <div class="control-group">
          <label for="prompt">Agent Instructions:</label>
          <textarea id="prompt" placeholder="Describe how you want your voice agent to behave...">You are a helpful, concise voice assistant. Keep responses brief and natural for voice conversation. You CAN generate images when users request them. When users ask for image generation, simply acknowledge that you're creating the image.</textarea>
        </div>
        
        <div class="control-group">
          <label>Voice Selection:</label>
          <div class="voice-options">
            <div class="voice-option selected" data-voice="alloy">
              <div>‚ö° Alloy</div>
              <small>Clear, energetic</small>
            </div>
            <div class="voice-option" data-voice="echo">
              <div>üåä Echo</div>
              <small>Warm, friendly</small>
            </div>
            <div class="voice-option" data-voice="fable">
              <div>üìö Fable</div>
              <small>Storyteller</small>
            </div>
            <div class="voice-option" data-voice="onyx">
              <div>üíé Onyx</div>
              <small>Deep, rich</small>
            </div>
            <div class="voice-option" data-voice="nova">
              <div>‚≠ê Nova</div>
              <small>Bright, cheerful</small>
            </div>
            <div class="voice-option" data-voice="shimmer">
              <div>‚ú® Shimmer</div>
              <small>Soft, gentle</small>
            </div>
          </div>
        </div>
      </div>
      
      <button id="startChat" class="btn">Start Voice Chat</button>
      <button id="stopChat" class="btn secondary" style="display: none;">Stop Chat</button>
      
      
      <div id="micControls" style="display: none; text-align: center; margin: 20px 0;">
        <button id="micButton" class="btn" style="width: 80px; height: 80px; border-radius: 50%; font-size: 24px; background: #28a745; display: flex; align-items: center; justify-content: center; margin: 0 auto;">
          <svg width="32" height="32" viewBox="0 0 24 24" fill="white" xmlns="http://www.w3.org/2000/svg">
            <path d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3z"/>
            <path d="M17 11c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z"/>
          </svg>
        </button>

        
        <div id="micStatus" style="margin-top: 10px; font-weight: bold; color: #155724;">Click to speak</div>
        <button id="retryButton" class="btn" style="display: none; margin-top: 10px; padding: 10px 20px; font-size: 14px; background: #ffc107; color: #000;">
          üîÑ Try Again
        </button>
        <button id="testMicButton" class="btn" style="margin-top: 10px; padding: 10px 20px; font-size: 14px; background: #d4edda; color: #155724; font-weight: bold;">
          üé§ Test Microphone
        </button>
      </div>
      
      <div class="conversation" id="conversation" style="display: none;">
        <div class="message assistant">
          <div class="message-label">Assistant</div>
          <div>Hello! I'm ready to chat. Click the microphone button to start speaking.</div>
        </div>
      </div>
      
      <div class="instructions">
        <h3>How to use:</h3>
        <ul>
          <li>Click "Start Voice Chat" to begin</li>
          <li>Allow microphone access when prompted</li>
          <li>Speak your message and wait for the response</li>
          <li>The AI will respond with text and voice</li>
          <li><strong>üé® Generate Images:</strong> Say "Generate image of..." or "Create a picture of..."</li>
          <li>Click on images to view full-size</li>
          <li>Download generated images with the download button</li>
          <li>Click "Stop Chat" when finished</li>
        </ul>
      </div>
    </div>

    <script>
      let isRecording = false;
      let recognition = null;
      let conversationHistory = [];
      let userInteracted = false;
      let micPaused = false;
      let recognitionTimeout = null;

      // Detect Edge browser and show warning
      if (navigator.userAgent.includes('Edg')) {
        document.getElementById('edgeWarning').style.display = 'block';
        
        // Enable audio on first user interaction
        document.addEventListener('click', () => {
          if (!userInteracted) {
            userInteracted = true;
            document.getElementById('edgeWarning').style.display = 'none';
            console.log('User interaction detected - audio enabled for Edge');
          }
        }, { once: true });
      }

      // Voice selection handling
      document.querySelectorAll('.voice-option').forEach(option => {
        option.addEventListener('click', () => {
          document.querySelectorAll('.voice-option').forEach(opt => opt.classList.remove('selected'));
          option.classList.add('selected');
        });
      });

      function updateStatus(message, type = 'default') {
        const statusEl = document.getElementById('status');
        const statusContainer = document.getElementById('statusContainer');
        
        statusEl.textContent = message;
        statusContainer.className = 'status-container';
        
        if (type === 'connected') {
          statusContainer.classList.add('connected');
        } else if (type === 'error') {
          statusContainer.classList.add('error');
        } else if (type === 'connecting') {
          statusContainer.classList.add('connecting');
        }
      }

      function updateMicStatus(message, isRecording = false) {
        const micStatus = document.getElementById('micStatus');
        const micButton = document.getElementById('micButton');
        
        micStatus.textContent = message;
        
        if (isRecording) {
          micButton.style.background = '#dc3545';
          micButton.innerHTML = '<svg width="32" height="32" viewBox="0 0 24 24" fill="white" xmlns="http://www.w3.org/2000/svg"><rect x="6" y="6" width="12" height="12" rx="2"/></svg>';
        } else {
          micButton.style.background = '#28a745';
          micButton.innerHTML = '<svg width="32" height="32" viewBox="0 0 24 24" fill="white" xmlns="http://www.w3.org/2000/svg"><path d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3z"/><path d="M17 11c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z"/></svg>';
        }
      }

      function getSelectedVoice() {
        const selected = document.querySelector('.voice-option.selected');
        return selected ? selected.dataset.voice : 'alloy';
      }

      async function testMicrophone() {
        try {
          console.log('Testing microphone with enhanced settings...');
          const stream = await navigator.mediaDevices.getUserMedia({ 
            audio: {
              echoCancellation: true,
              noiseSuppression: true,
              autoGainControl: true,
              sampleRate: 44100,
              channelCount: 1
            }
          });
          
          console.log('Microphone test successful with enhanced settings');
          updateStatus('Microphone is working - try speaking clearly and close to mic', 'connected');
          
          // Stop the test stream
          stream.getTracks().forEach(track => track.stop());
          
        } catch (error) {
          console.error('Microphone test failed:', error);
          updateStatus('Microphone test failed - check your microphone', 'error');
        }
      }

      function addMessage(role, content) {
        const conversation = document.getElementById('conversation');
        const messageDiv = document.createElement('div');
        messageDiv.className = `message ${role}`;
        messageDiv.innerHTML = `
          <div class="message-label">${role === 'user' ? 'You' : 'Assistant'}</div>
          <div>${content}</div>
        `;
        conversation.appendChild(messageDiv);
        conversation.scrollTop = conversation.scrollHeight;
      }

      async function startChat() {
        try {
          updateStatus('Setting up voice chat...', 'connecting');
          
          // Check if speech recognition is supported
          if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
            throw new Error('Speech recognition not supported in this browser. Please use Chrome or Edge.');
          }
          
          // Test microphone access first with better audio settings
          try {
            const stream = await navigator.mediaDevices.getUserMedia({ 
              audio: {
                echoCancellation: true,
                noiseSuppression: true,
                autoGainControl: true,
                sampleRate: 44100
              }
            });
            console.log('Microphone access granted with enhanced settings');
            stream.getTracks().forEach(track => track.stop()); // Stop the test stream
            updateStatus('Microphone access confirmed', 'connecting');
          } catch (micError) {
            console.error('Microphone access denied:', micError);
            throw new Error('Microphone access denied. Please allow microphone access and refresh the page.');
          }
          
          // Initialize speech recognition with Edge-specific settings
          const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
          recognition = new SpeechRecognition();
          
          // Force Edge to use the recognition properly
          if (navigator.userAgent.includes('Edg')) {
            console.log('Applying Edge-specific speech recognition fixes');
            // Force recognition to work in Edge
            recognition.continuous = false;
            recognition.interimResults = false;
            
            // Edge-specific timeout and retry settings
            recognition.maxAlternatives = 1;
            
            // Add Edge-specific event listeners
            recognition.onstart = () => {
              console.log('Edge: Speech recognition started');
              isRecording = true;
              updateMicStatus('Listening...', true);
              updateStatus('Listening... Speak now!', 'connecting');
              
              // Longer timeout for Edge
              recognitionTimeout = setTimeout(() => {
                if (recognition && isRecording) {
                  console.log('Edge: Recognition timeout');
                  recognition.stop();
                  updateStatus('Listening timeout - try again', 'warning');
                }
              }, 25000); // 25 seconds for Edge
            };
          }
          
          // Edge-specific configuration
          if (navigator.userAgent.includes('Edg')) {
            console.log('Detected Edge browser - applying Edge-specific settings');
          }
          
          // Add echo cancellation warning
          updateStatus('üí° Tip: Use headphones to prevent echo, or lower speaker volume', 'connecting');
          
          recognition.continuous = false;
          recognition.interimResults = false; // Disable interim results to avoid confusion
          recognition.lang = 'en-US';
          recognition.maxAlternatives = 1; // Use single best result
          
          // Try different language variants for better recognition
          if (navigator.language && navigator.language.startsWith('en')) {
            recognition.lang = navigator.language;
          }
          
          recognition.onstart = () => {
            isRecording = true;
            updateMicStatus('Listening...', true);
            updateStatus('Listening... Speak now!', 'connecting');
            
            // Set timeout to stop recognition after 20 seconds (increased for better recognition)
            recognitionTimeout = setTimeout(() => {
              if (recognition && isRecording) {
                recognition.stop();
                updateStatus('Listening timeout - try again', 'warning');
              }
            }, 20000);
          };
          
          recognition.onresult = async (event) => {
            // Get the final result
            const result = event.results[event.results.length - 1];
            if (result.isFinal) {
              const transcript = result[0].transcript.trim();
              const confidence = result[0].confidence;
              
              console.log('Final result:', transcript, 'Confidence:', confidence);
              
              // Process if we have text - lower threshold for better recognition
              if (transcript !== '' && confidence > 0.05) {
                console.log('Processing result:', transcript, 'Confidence:', confidence);
                await processMessage(transcript);
              } else if (transcript !== '' && transcript.length > 3) {
                // Fallback: if we have reasonable text but low confidence, still try to process
                console.log('Low confidence but processing anyway:', transcript, 'Confidence:', confidence);
                await processMessage(transcript);
              } else if (transcript !== '') {
                console.log('Very low confidence result ignored:', transcript, 'Confidence:', confidence);
                updateStatus('Speech unclear - please speak more clearly and closer to microphone', 'error');
                document.getElementById('retryButton').style.display = 'block';
              } else {
                console.log('No transcript found in results');
                updateStatus('No speech detected - please try again', 'error');
                document.getElementById('retryButton').style.display = 'block';
              }
            }
          };
          
          recognition.onerror = (event) => {
            console.error('Speech recognition error:', event.error);
            
            let errorMessage = 'Speech recognition error: ' + event.error;
            let micMessage = 'Error: ' + event.error;
            
            if (event.error === 'no-speech') {
              errorMessage = 'No speech detected. Try speaking louder, closer to microphone, or check microphone settings.';
              micMessage = 'No speech detected - try again';
              // Show retry button for no-speech errors
              document.getElementById('retryButton').style.display = 'block';
              
              // Test microphone again and try different approach
              testMicrophone();
              
              // Edge-specific retry logic
              if (navigator.userAgent.includes('Edg')) {
                console.log('Edge: No-speech error, attempting aggressive retry');
                // More aggressive retry for Edge
                setTimeout(async () => {
                  if (recognition && !isRecording) {
                    try {
                      // Force microphone access again for Edge
                      const stream = await navigator.mediaDevices.getUserMedia({ 
                        audio: {
                          echoCancellation: true,
                          noiseSuppression: true,
                          autoGainControl: true
                        }
                      });
                      stream.getTracks().forEach(track => track.stop());
                      console.log('Edge: Forced microphone access for retry');
                      
                      // Wait a bit longer for Edge
                      setTimeout(() => {
                        if (recognition && !isRecording) {
                          console.log('Edge: Restarting recognition after forced mic access');
                          recognition.start();
                        }
                      }, 1500);
                    } catch (error) {
                      console.error('Edge: Failed to force microphone access for retry:', error);
                    }
                  }
                }, 3000); // Longer delay for Edge
              } else {
                // Standard retry for other browsers
                setTimeout(() => {
                  if (recognition && !isRecording) {
                    console.log('Attempting to restart recognition after no-speech error');
                    recognition.start();
                  }
                }, 2000);
              }
            } else if (event.error === 'not-allowed') {
              errorMessage = 'Microphone permission denied. Please allow microphone access.';
              micMessage = 'Permission denied';
            } else if (event.error === 'network') {
              errorMessage = 'Network error. Check your internet connection.';
              micMessage = 'Network error';
            } else if (event.error === 'audio-capture') {
              errorMessage = 'Microphone not found or not working. Check your microphone.';
              micMessage = 'Microphone error';
            } else if (event.error === 'service-not-allowed') {
              errorMessage = 'Speech recognition service not allowed. Try refreshing the page.';
              micMessage = 'Service error';
            }
            
            updateMicStatus(micMessage, false);
            updateStatus(errorMessage, 'error');
            isRecording = false;
          };
          
          recognition.onend = () => {
            isRecording = false;
            updateMicStatus('Click to speak', false);
            updateStatus('Voice chat ready! Click microphone to speak', 'connected');
            
            // Clear timeout
            if (recognitionTimeout) {
              clearTimeout(recognitionTimeout);
              recognitionTimeout = null;
            }
          };
          
          // Add speech detection events
          recognition.onspeechstart = () => {
            console.log('Speech started');
            updateMicStatus('Speech detected...', true);
            updateStatus('Speech detected - processing...', 'connecting');
          };
          
          recognition.onspeechend = () => {
            console.log('Speech ended');
            updateMicStatus('Processing speech...', true);
            updateStatus('Processing your speech...', 'connecting');
          };
          
          recognition.onsoundstart = () => {
            console.log('Sound detected');
            updateMicStatus('Sound detected...', true);
          };
          
          recognition.onsoundend = () => {
            console.log('Sound ended');
          };
          
          document.getElementById('startChat').style.display = 'none';
          document.getElementById('stopChat').style.display = 'block';
          document.getElementById('conversation').style.display = 'block';
          document.getElementById('micControls').style.display = 'block';
          
          updateStatus('Voice chat ready! Click microphone to speak', 'connected');
          
        } catch (error) {
          console.error('Error setting up voice chat:', error);
          updateStatus(`Error: ${error.message}`, 'error');
        }
      }

      async function stopChat() {
        if (recognition) {
          recognition.stop();
        }
        
        document.getElementById('startChat').style.display = 'block';
        document.getElementById('stopChat').style.display = 'none';
        document.getElementById('conversation').style.display = 'none';
        document.getElementById('micControls').style.display = 'none';
        
        updateStatus('Chat stopped', 'default');
        isRecording = false;
      }

      async function processMessage(text) {
        try {
          console.log('processMessage called with:', text);
          updateStatus('Processing your message...', 'connecting');
          
          if (!text || text.trim() === '') {
            console.log('Empty text received');
            updateStatus('No speech detected. Please try again.', 'error');
            return;
          }
          
          console.log('Adding user message to conversation');
          addMessage('user', text);
          
          // Check if this is an image generation request
          const imageKeywords = [
            'generate image', 'generate an image', 'generate a image',
            'create image', 'create an image', 'create a image', 
            'make image', 'make an image', 'make a image',
            'draw', 'paint', 'sketch',
            'generate a picture', 'generate an picture', 'generate picture',
            'create a picture', 'create an picture', 'create picture',
            'make a picture', 'make an picture', 'make picture',
            'generate photo', 'create photo', 'make photo',
            'generate a', 'create a', 'make a', 'draw a', 'paint a'
          ];
          const isImageRequest = imageKeywords.some(keyword => 
            text.toLowerCase().includes(keyword.toLowerCase())
          );
          
          if (isImageRequest) {
            console.log('Image generation request detected');
            // Extract the image prompt from the text
            let imagePrompt = text;
            
            // Remove common phrases to get the actual prompt
            imageKeywords.forEach(keyword => {
              const regex = new RegExp(keyword, 'gi');
              imagePrompt = imagePrompt.replace(regex, '').trim();
            });
            
            // Clean up the prompt - remove common prefixes
            imagePrompt = imagePrompt.replace(/^(of|a|an|the|with|showing|depicting|featuring)\s+/i, '').trim();
            
            // If prompt is still empty, try to extract from the original text
            if (!imagePrompt) {
              // Look for patterns like "of [something]" or "with [something]"
              const match = text.match(/(?:of|with|showing|depicting|featuring)\s+(.+)/i);
              if (match) {
                imagePrompt = match[1].trim();
              }
            }
            
            if (imagePrompt) {
              await generateImage(imagePrompt);
              return;
            } else {
              addMessage('assistant', 'I\'d be happy to generate an image for you! Please describe what you\'d like me to create.');
              await textToSpeech('I\'d be happy to generate an image for you! Please describe what you\'d like me to create.', getSelectedVoice());
              return;
            }
          }
          
          // Send to GPT API
          console.log('Sending request to /chat endpoint');
          const response = await fetch('/chat', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
              message: text,
              instructions: document.getElementById('prompt').value,
              history: conversationHistory
            })
          });
          
          console.log('Chat API response status:', response.status);
          
          if (!response.ok) {
            const errorText = await response.text();
            console.error('Chat API error:', errorText);
            throw new Error(`Chat API error: ${response.statusText}`);
          }
          
          const data = await response.json();
          console.log('Chat API response data:', data);
          addMessage('assistant', data.response);
          
          // Convert response to speech
          console.log('Converting response to speech:', data.response);
          await textToSpeech(data.response, getSelectedVoice());
          
          // Update conversation history
          conversationHistory.push({ role: 'user', content: text });
          conversationHistory.push({ role: 'assistant', content: data.response });
          
          updateStatus('Voice chat ready! Click microphone to speak', 'connected');
          
        } catch (error) {
          console.error('Error processing message:', error);
          updateStatus(`Error: ${error.message}`, 'error');
        }
      }

      async function generateImage(prompt) {
        try {
          console.log('Generating image for prompt:', prompt);
          updateStatus('üé® Generating image...', 'connecting');
          
          // Add a message to show we're generating
          addMessage('assistant', 'üé® Creating your image...');
          
          const response = await fetch('/generate-image', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ prompt })
          });
          
          console.log('Image generation response status:', response.status);
          
          if (!response.ok) {
            const errorText = await response.text();
            console.error('Image generation error:', errorText);
            throw new Error(`Image generation error: ${response.statusText}`);
          }
          
          const data = await response.json();
          console.log('Image generation response data:', data);
          
          // Display the generated image
          displayGeneratedImage(data.imageUrl, data.prompt, data.revisedPrompt);
          
          updateStatus('üé® Image generated successfully!', 'connected');
          
        } catch (error) {
          console.error('Error generating image:', error);
          updateStatus(`Image generation error: ${error.message}`, 'error');
          addMessage('assistant', `Sorry, I couldn't generate the image: ${error.message}`);
        }
      }

      function displayGeneratedImage(imageUrl, originalPrompt, revisedPrompt) {
        const conversation = document.getElementById('conversation');
        const messageDiv = document.createElement('div');
        messageDiv.className = 'message assistant';
        
        const imageId = 'img_' + Date.now();
        messageDiv.innerHTML = `
          <div class="message-label">Assistant</div>
          <div>Here's your generated image:</div>
          <img src="${imageUrl}" alt="Generated image" class="generated-image" id="${imageId}" onclick="openImageModal('${imageUrl}')">
          <div class="image-prompt">Prompt: ${revisedPrompt || originalPrompt}</div>
          <button class="download-btn" onclick="downloadImage('${imageUrl}', 'generated-image-${Date.now()}.png')">üì• Download</button>
        `;
        
        conversation.appendChild(messageDiv);
        conversation.scrollTop = conversation.scrollHeight;
      }

      function openImageModal(imageUrl) {
        // Create modal for full-size image view
        const modal = document.createElement('div');
        modal.style.cssText = `
          position: fixed; top: 0; left: 0; width: 100%; height: 100%; 
          background: rgba(0,0,0,0.8); display: flex; align-items: center; 
          justify-content: center; z-index: 1000; cursor: pointer;
        `;
        
        const img = document.createElement('img');
        img.src = imageUrl;
        img.style.cssText = 'max-width: 90%; max-height: 90%; border-radius: 8px;';
        
        modal.appendChild(img);
        document.body.appendChild(modal);
        
        modal.onclick = () => document.body.removeChild(modal);
      }

      function downloadImage(imageUrl, filename) {
        const link = document.createElement('a');
        link.href = imageUrl;
        link.download = filename;
        link.target = '_blank';
        document.body.appendChild(link);
        link.click();
        document.body.removeChild(link);
      }

      async function textToSpeech(text, voice) {
        try {
          console.log('textToSpeech called with:', text, 'voice:', voice);
          
          // Stop speech recognition while AI is speaking to prevent echo
          if (recognition && isRecording) {
            recognition.stop();
            isRecording = false;
            updateMicStatus('AI is speaking...', false);
          }
          updateStatus('üîä AI is speaking...', 'connecting');
          
          console.log('Sending request to /speak endpoint');
          const response = await fetch('/speak', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ text, voice })
          });
          
          console.log('TTS API response status:', response.status);
          
          if (!response.ok) {
            const errorText = await response.text();
            console.error('TTS API error:', errorText);
            throw new Error(`TTS error: ${response.statusText}`);
          }
          
          const audioBlob = await response.blob();
          const audio = new Audio(URL.createObjectURL(audioBlob));
          
          // Add event listeners to manage speech recognition during playback
          audio.addEventListener('play', () => {
            updateStatus('üîä AI is speaking...', 'connecting');
          });
          
          audio.addEventListener('ended', () => {
            updateStatus('Voice chat ready! Click microphone to speak', 'connected');
            updateMicStatus('Click to speak', false);
            
            // Auto-restart recognition after a short delay to prevent echo
            setTimeout(() => {
              if (recognition && !isRecording && !micPaused) {
                updateStatus('Ready for next message - click microphone to speak', 'connected');
              }
            }, 1000);
          });
          
          // Edge-specific audio handling
          if (navigator.userAgent.includes('Edg')) {
            console.log('Playing audio in Edge browser');
            // Ensure audio is unmuted and volume is set
            audio.muted = false;
            audio.volume = 1.0;
            
            // Edge-specific audio settings
            audio.preload = 'auto';
            audio.crossOrigin = 'anonymous';
            
            // Try to play immediately first
            try {
              await audio.play();
              console.log('Edge: Audio played successfully');
            } catch (e) {
              console.log('Initial audio play failed in Edge, trying with user interaction...');
              
              // If immediate play fails, wait for user interaction
              const playAudio = () => {
                audio.play().catch(err => {
                  console.error('Edge audio play error:', err);
                  updateStatus('Audio playback failed. Text response is still available.', 'error');
                });
              };
              
              if (userInteracted) {
                playAudio();
              } else {
                // Wait for user interaction
                document.addEventListener('click', playAudio, { once: true });
                updateStatus('Click anywhere to enable audio playback', 'connecting');
              }
            }
          } else {
            await audio.play();
          }
          
        } catch (error) {
          console.error('Error with text-to-speech:', error);
          updateStatus(`Audio error: ${error.message}`, 'error');
        }
      }

      // Add click handlers
      document.addEventListener('click', async (e) => {
        if (e.target.id === 'startChat') {
          await startChat();
        } else if (e.target.id === 'stopChat') {
          await stopChat();
        } else if (e.target.id === 'micButton') {
          if (!isRecording && recognition && !micPaused) {
            document.getElementById('retryButton').style.display = 'none';
            updateStatus('Starting microphone...', 'connecting');
            
            // Add a longer delay and force microphone access for Edge
            setTimeout(async () => {
              if (recognition && !isRecording) {
                try {
                  // Edge-specific microphone settings
                  const audioConstraints = navigator.userAgent.includes('Edg') ? {
                    audio: {
                      echoCancellation: true,
                      noiseSuppression: true,
                      autoGainControl: true,
                      sampleRate: 44100,
                      channelCount: 1,
                      volume: 1.0
                    }
                  } : {
                    audio: {
                      echoCancellation: true,
                      noiseSuppression: true,
                      autoGainControl: true
                    }
                  };
                  
                  // Force microphone access before starting recognition
                  const stream = await navigator.mediaDevices.getUserMedia(audioConstraints);
                  stream.getTracks().forEach(track => track.stop());
                  
                  console.log('Forced microphone access before recognition');
                  
                  // Edge needs a bit more time
                  const delay = navigator.userAgent.includes('Edg') ? 1500 : 500;
                  setTimeout(() => {
                    if (recognition && !isRecording) {
                      recognition.start();
                    }
                  }, delay);
                  
                } catch (error) {
                  console.error('Failed to force microphone access:', error);
                  recognition.start(); // Try anyway
                }
              }
            }, navigator.userAgent.includes('Edg') ? 1500 : 1000);
          } else if (micPaused) {
            updateStatus('Microphone is paused - click "Resume Mic" first', 'warning');
          }
        } else if (e.target.id === 'retryButton') {
          if (!isRecording && recognition) {
            document.getElementById('retryButton').style.display = 'none';
            updateStatus('Retrying speech recognition...', 'connecting');
            recognition.start();
          }
        } else if (e.target.id === 'testMicButton') {
          await testMicrophone();
        } else if (e.target.id === 'pauseMic') {
          if (micPaused) {
            micPaused = false;
            e.target.textContent = '‚è∏Ô∏è Pause Mic (if echoing)';
            e.target.style.background = '#dc3545';
            updateStatus('Microphone unpaused', 'connected');
          } else {
            micPaused = true;
            if (recognition && isRecording) {
              recognition.stop();
            }
            e.target.textContent = '‚ñ∂Ô∏è Resume Mic';
            e.target.style.background = '#28a745';
            updateStatus('Microphone paused - click to resume', 'warning');
          }
        }
      });
    </script>
  </body>
</html>
