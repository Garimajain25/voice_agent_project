<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>GPT Voice Agent (Realtime Version)</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>üé§</text></svg>">
    <style>
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
        background: linear-gradient(135deg,rgb(3, 80, 16) 0%,rgb(202, 235, 190) 100%);
        min-height: 100vh;
        display: flex;
        align-items: center;
        justify-content: center;
        padding: 20px;
      }
      
      .container {
        background: white;
        border-radius: 20px;
        box-shadow: 0 20px 40px rgba(0,0,0,0.1);
        padding: 40px;
        max-width: 600px; 
        width: 100%;
        text-align: center;
      }
      
      h1 {
        color: #155724;
        margin-bottom: 10px;
        font-size: 3rem;
        font-weight: 700;
      }
      
      .subtitle {
        color: #155724;
        margin-bottom: 30px;
        font-size: 1.1rem;
      }
      
      .status-container {
        margin: 20px 0;
        padding: 15px;
        border-radius: 10px;
        background: #d4edda;
        border: 2px solid #c3e6cb;
        transition: all 0.3s ease;
        color: #155724;
      }
      
      .status-container.connected {
        background: #d4edda;
        border-color: #c3e6cb;
        color: #155724;
      }
      
      .status-container.error {
        background: #f8d7da;
        border-color: #f5c6cb;
        color: #721c24;
      }
      
      .status-container.connecting {
        background: #fff3cd;
        border-color: #ffeaa7;
        color: #856404;
      }
      
      #status {
        font-weight: 600;
        font-size: 1.1rem;
      }
      
      .controls {
        margin: 30px 0;
        text-align: left;
      }
      
      .control-group {
        margin-bottom: 20px;
      }
      
      label {
        display: block;
        margin-bottom: 8px;
        font-weight: 600;
        color: #155724;
        font-weight: bold;
      }
      
      textarea {
        width: 100%;
        padding: 12px;
        border: 2px solid #155724;
        border-radius: 8px;
        font-size: 16px;
        transition: border-color 0.3s ease;
        color: #155724;
        resize: vertical;
        min-height: 80px;
      }
      
      textarea:focus {
        outline: none;
        border-color: #667eea;
      }
    
      .btn {
        background: #d4edda;
        color: #155724;
        border: none;
        padding: 15px 30px;
        font-size: 18px;
        font-weight: bold;
        border-radius: 50px;
        cursor: pointer;
        transition: all 0.3s ease;
        width: 100%;
        margin: 10px 0;
      }
      
      .btn:hover {
        transform: translateY(-2px);
        box-shadow: 0 10px 20px rgba(102, 126, 234, 0.3);
      }
      
      .btn:disabled {
        background: #ccc;
        cursor: not-allowed;
        transform: none;
        box-shadow: none;
      }
      
      .btn.secondary {
        background: #155724;
        color: white;
      }
      
      .conversation {
        margin: 20px 0;
        max-height: 400px;  
        overflow-y: auto;
        overflow-x: hidden;
        border: 1px solid #e0e0e0;
        border-radius: 10px;
        padding: 15px;
        text-align: left;
        background: #f8f9fa;
        word-wrap: break-word;
      }
      
      .message {
        margin: 10px 0;
        padding: 10px;
        border-radius: 8px;
        overflow: hidden;
        word-wrap: break-word;
      }
      
      .message.user {
        background: linear-gradient(135deg, #d4edda 0%, #c3e6cb 100%);
        margin-left: 20px;
        color: #155724;
        font-weight: bold;
        border: 2px solid #28a745;
        box-shadow: 0 4px 8px rgba(40, 167, 69, 0.2);
      }
      
      .message.assistant {
        background: #155724;
        margin-right: 20px;
        color: white;
        box-shadow: 0 4px 8px rgba(40, 167, 69, 0.3);
      }
      
      .message-label {
        font-weight: 600;
        font-size: 12px;
        color: #155724;
        margin-bottom: 5px;
      }
      
      .message.assistant .message-label {
        color: white;
      }
      
      .mic-button {
        width: 80px;
        height: 80px;
        border-radius: 50%;
        border: none;
        background: #28a745;
        color: white;
        font-size: 24px;
        cursor: pointer;
        transition: all 0.3s ease;
        display: flex;
        align-items: center;
        justify-content: center;
        margin: 20px auto;
        box-shadow: 0 8px 16px rgba(40, 167, 69, 0.3);
      }
      
      .mic-button:hover {
        transform: scale(1.05);
        box-shadow: 0 12px 24px rgba(40, 167, 69, 0.4);
      }
      
      .mic-button.recording {
        background: #dc3545;
        animation: pulse 1.5s infinite;
      }
      
      @keyframes pulse {
        0% { transform: scale(1); }
        50% { transform: scale(1.1); }
        100% { transform: scale(1); }
      }
      
      .mic-button:disabled {
        background: #ccc;
        cursor: not-allowed;
        transform: none;
        animation: none;
      }
      
      .latency-indicator {
        position: fixed;
        top: 20px;
        right: 20px;
        background: rgba(0,0,0,0.8);
        color: white;
        padding: 10px 15px;
        border-radius: 20px;
        font-size: 14px;
        font-weight: bold;
      }
      
      .latency-indicator.low {
        background: rgba(76, 175, 80, 0.9);
      }
      
      .latency-indicator.medium {
        background: rgba(255, 193, 7, 0.9);
      }
      
      .latency-indicator.high {
        background: rgba(244, 67, 54, 0.9);
      }
      
      .instructions {
        margin-top: 20px;
        padding: 15px;
        background: #f8f9fa;
        border-radius: 10px;
        font-size: 14px;
        color: #155724;
        text-align: left;
      }
      
      .instructions h3 {
        margin-bottom: 10px;
        color: #155724;
      }
      
      .instructions ul {
        margin-left: 20px;
      }
      
      .instructions li {
        margin-bottom: 5px;
      }
      
      .voice-options {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(120px, 1fr));
        gap: 10px;
        margin-top: 10px;
      }
      
      .voice-option {
        padding: 10px;
        border: 2px solid #e9ecef;
        border-radius: 8px;
        cursor: pointer;
        transition: all 0.3s ease;
        text-align: center;
        color: #155724;
      }
      
      .voice-option:hover {
        border-color: #667eea;
        background: #f8f9ff;
      }
      
      .voice-option.selected {
        border-color: #c3e6cb;
        background: #d4edda;
        color: #155724;
        font-weight: bold;
      }
      
      @media (max-width: 600px) {
        .container {
          padding: 20px;
          margin: 10px;
        }
        
        h1 {
          font-size: 2rem;
        }
      }
    </style>
  </head>
  <body>
    <div class="latency-indicator" id="latencyIndicator">
      Latency: --ms
    </div>
    
    <div class="container">
      <h1>üé§ Voice Agent</h1>
      <p class="subtitle">Powered by OpenAI Realtime API</p>
      
      <div class="status-container" id="statusContainer">
        <div id="status">Ready to start real-time conversation</div>
      </div>
      
      <div class="controls">
        <div class="control-group">
          <label for="instructions">Agent Instructions:</label>
          <textarea id="instructions" placeholder="Describe how you want your voice agent to behave...">You are a helpful, fast voice assistant. Speak quickly and efficiently with a confident, clear tone. Keep responses brief and direct. Use a fast, clear, and efficient voice. Be quick and to the point in your responses. Speak at a faster pace than normal. Only respond in English language.</textarea>
        </div>
        
        <div class="control-group">
          <label>Voice Selection:</label>
          <div class="voice-options">
            <div class="voice-option selected" data-voice="alloy">
              <div>üîä Alloy</div>
              <small>Clear, energetic</small>
            </div>
            <div class="voice-option" data-voice="echo">
              <div>üí¨ Echo</div>
              <small>Warm, friendly</small>
            </div>
            <div class="voice-option" data-voice="shimmer">
              <div>‚ú® Shimmer</div>
              <small>Soft, gentle</small>
            </div>
            <div class="voice-option" data-voice="fable">
              <div>üìñ Fable</div>
              <small>Storyteller</small>
            </div>
            <div class="voice-option" data-voice="onyx">
              <div>üíé Onyx</div>
              <small>Deep, rich</small>
            </div>
            <div class="voice-option" data-voice="nova">
              <div>‚≠ê Nova</div>
              <small>Bright, cheerful</small>
            </div>
          </div>
        </div>
      </div>
      
      <button id="startSession" class="btn">Start Voice Chat</button>
      <button id="stopSession" class="btn secondary" style="display: none;">Stop Chat</button>
      
      <button id="micButton" class="mic-button" style="display: none;" disabled>
        üé§
      </button>
      
      
      <div class="conversation" id="conversation" style="display: none;">
        <div class="message assistant">
          <div class="message-label">Assistant</div>
          <div>Hello! I'm ready for real-time conversation. Click the microphone to start speaking.</div>
        </div>
      </div>
      
      <div class="instructions">
        <h3>How to use:</h3>
        <ul>
          <li>Click "Start Voice Chat" to begin</li>
          <li>Allow microphone access when prompted</li>
          <li>Click the microphone button to start speaking</li>
          <li>Speak your message clearly (e.g., "provide me a good morning text")</li>
          <li>The system will automatically detect when you stop speaking</li>
          <li>The AI will respond with text and voice</li>
          <li>Click "Stop Chat" when finished</li>
        </ul>
      </div>
    </div>

    <script>
      let ws = null;
      let isConnected = false;
      let isRecording = false;
      let mediaRecorder = null;
      let audioContext = null;
      let startTime = null;
      let latencyHistory = [];
      let audioProcessor = null;
      let isSpeaking = false;
      let responseTimeout = null;
      let selectedVoice = 'alloy'; // Default to alloy voice

      // Initialize WebSocket connection
      function connectWebSocket() {
        const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
        const wsUrl = `${protocol}//${window.location.host}`;
        
        updateStatus('Connecting to server...', 'connecting');
        ws = new WebSocket(wsUrl);
        
        ws.onopen = () => {
          console.log('WebSocket connected');
          updateStatus('Connected - initializing realtime session...', 'connecting');
          
          // Start the realtime session
          console.log('Starting session with voice:', selectedVoice);
          ws.send(JSON.stringify({
            type: 'start_session',
            instructions: document.getElementById('instructions').value,
            voice: selectedVoice
          }));
        };
        
        ws.onmessage = (event) => {
          const data = JSON.parse(event.data);
          handleWebSocketMessage(data);
        };
        
        ws.onclose = (event) => {
          console.log('WebSocket disconnected:', event.code, event.reason);
          if (event.code !== 1000) { // Not a normal closure
            updateStatus('Connection lost - please restart session', 'error');
          } else {
            updateStatus('Session ended', 'default');
          }
          isConnected = false;
          stopRecording();
        };
        
        ws.onerror = (error) => {
          console.error('WebSocket error:', error);
          updateStatus('Connection error - please check your network and try again', 'error');
        };
      }

      // Handle WebSocket messages
      function handleWebSocketMessage(data) {
        switch (data.type) {
          case 'session_started':
            updateStatus('‚úÖ Realtime session active - click microphone to speak!', 'connected');
            isConnected = true;
            document.getElementById('startSession').style.display = 'none';
            document.getElementById('stopSession').style.display = 'block';
            document.getElementById('micButton').style.display = 'block';
            document.getElementById('conversation').style.display = 'block';
            document.getElementById('micButton').disabled = false;
            startAudioCapture();
            break;
            
          case 'audio_response':
            console.log('Received audio response, data:', data);
            console.log('Audio data type:', typeof data.audio);
            console.log('Audio data length:', data.audio ? data.audio.length : 'undefined');
            bufferAudioChunk(data.audio);
            break;
            
          case 'text_response':
            console.log('Received text response:', data.text);
            console.log('About to call addMessage with role: assistant');
            addMessage('assistant', data.text, true);
            break;
            
          case 'transcription':
            console.log('Received transcription:', data.text);
            console.log('About to call addMessage with role: user for transcription');
            addMessage('user', data.text, true);
            break;
            
          case 'transcription_complete':
            addMessage('user', data.text);
            break;
            
          case 'transcription_delta':
            addMessage('user', data.text, true);
            break;
            
          case 'speech_started':
            updateStatus('üé§ Speech detected...', 'connecting');
            break;
            
          case 'speech_stopped':
            updateStatus('Processing speech...', 'connecting');
            break;
            
          case 'audio_committed':
            updateStatus('Audio processed, waiting for response...', 'connecting');
            break;
            
          case 'response_created':
            console.log('Response created - AI is responding');
            updateStatus('AI is responding...', 'connecting');
            // Clear response timeout
            if (responseTimeout) {
              clearTimeout(responseTimeout);
              responseTimeout = null;
            }
            
            // Set a fallback timeout to clear the status if response_complete is not received
            setTimeout(() => {
              const currentStatus = document.getElementById('status').textContent;
              if (currentStatus === 'AI is responding...') {
                console.log('Fallback: Clearing AI responding status');
                updateStatus('Ready - click microphone to speak', 'connected');
              }
            }, 10000); // 10 second fallback
            break;
            
          case 'response_complete':
            console.log('Response complete - clearing status');
            updateLatency();
            updateStatus('Ready - click microphone to speak', 'connected');
            // Clear response timeout
            if (responseTimeout) {
              clearTimeout(responseTimeout);
              responseTimeout = null;
            }
            // Play buffered audio immediately for faster response (only if not already playing)
            setTimeout(() => {
              if (!isPlayingAudio && audioChunks.length > 0) {
                playBufferedAudio();
              }
            }, 200);
            break;
            
          case 'session_ended':
            updateStatus('Session ended', 'default');
            isConnected = false;
            stopRecording();
            break;
            
          case 'error':
            updateStatus(`Error: ${data.message}`, 'error');
            break;
        }
      }

      // Start audio capture
      async function startAudioCapture() {
        try {
          const stream = await navigator.mediaDevices.getUserMedia({
            audio: {
              echoCancellation: true,
              noiseSuppression: true,
              autoGainControl: true,
              sampleRate: 16000,
              channelCount: 1,
              latency: 0.01,
              volume: 1.0
            }
          });
          
          audioContext = new (window.AudioContext || window.webkitAudioContext)({
            sampleRate: 16000
          });
          
          const source = audioContext.createMediaStreamSource(stream);
          
          // Use simple MediaRecorder without complex processing
          const mediaRecorder = new MediaRecorder(stream, {
            mimeType: 'audio/webm;codecs=opus'
          });
          
          let audioChunks = [];
          let isRecordingAudio = false;
          let recordingStartTime = null;
          
          mediaRecorder.ondataavailable = (event) => {
            if (event.data.size > 0) {
              audioChunks.push(event.data);
            }
          };
          
          mediaRecorder.onstop = () => {
            if (audioChunks.length > 0 && isConnected && ws && ws.readyState === WebSocket.OPEN) {
              const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
              
              // Convert WebM to PCM16 using AudioContext
              const reader = new FileReader();
              reader.onload = () => {
                const arrayBuffer = reader.result;
                
                // Send WebM audio directly (simpler approach)
                const base64Audio = btoa(String.fromCharCode(...new Uint8Array(arrayBuffer)));
                
                console.log('Sending WebM audio data, size:', arrayBuffer.byteLength, 'bytes');
                
                ws.send(JSON.stringify({
                  type: 'audio_data',
                  audio: base64Audio
                }));
                
                ws.send(JSON.stringify({
                  type: 'audio_complete'
                }));
                
                updateStatus('Audio sent - processing...', 'connecting');
                
                // Set response timeout
                responseTimeout = setTimeout(() => {
                  updateStatus('Response timeout - try again', 'error');
                  console.log('Response timeout after 30 seconds');
                }, 30000); // 30 second timeout
              };
              
              reader.readAsArrayBuffer(audioBlob);
              audioChunks = [];
            }
          };
          
          // Store mediaRecorder for cleanup
          window.currentMediaRecorder = mediaRecorder;
          
          // Add voice activity detection for microphone button
          const analyser = audioContext.createAnalyser();
          analyser.fftSize = 1024;
          analyser.smoothingTimeConstant = 0.3;
          const dataArray = new Uint8Array(analyser.frequencyBinCount);
          
          // Connect analyser directly to source
          source.connect(analyser);
          
          let isCurrentlyRecording = false;
          let silenceTimeout = null;
          let speechStartTime = null;
          let isListening = false; // Only listen when microphone button is clicked
          let maxRecordingTimeout = null; // Maximum recording time limit
          
          // Use Web Speech API for better speech recognition (like original system)
          const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
          recognition.continuous = false;
          recognition.interimResults = false;
          recognition.lang = 'en-US';
          
          recognition.onstart = () => {
            updateStatus('üé§ Listening... Speak now!', 'connecting');
            console.log('Speech recognition started');
          };
          
          recognition.onresult = (event) => {
            const transcript = event.results[0][0].transcript;
            console.log('Speech recognized:', transcript);
            
            // Add user message to conversation
            console.log('Adding user message:', transcript);
            addMessage('user', transcript);
            
            // Send text to AI instead of audio
            if (ws && ws.readyState === WebSocket.OPEN) {
              ws.send(JSON.stringify({
                type: 'text_input',
                text: transcript
              }));
              updateStatus('Processing text...', 'connecting');
              
              // Set response timeout for text input
              responseTimeout = setTimeout(() => {
                updateStatus('Response timeout - try again', 'error');
                console.log('Response timeout after 30 seconds');
              }, 30000); // 30 second timeout
            }
          };
          
          recognition.onerror = (event) => {
            console.error('Speech recognition error:', event.error);
            updateStatus('Speech recognition error - try again', 'error');
          };
          
          recognition.onend = () => {
            // Don't update status here as it might interfere with response status
            console.log('Speech recognition ended');
          };
          
          // Microphone button click handler
          document.getElementById('micButton').addEventListener('click', () => {
            if (recognition.state !== 'listening') {
              console.log('Microphone clicked - starting speech recognition');
              recognition.start();
            } else {
              console.log('Microphone clicked - stopping speech recognition');
              recognition.stop();
            }
          });
          
          // Function to stop recording
          function stopRecordingNow() {
            if (isCurrentlyRecording) {
              const recordingDuration = Date.now() - speechStartTime;
              
              // Check if recording is long enough (at least 1 second)
              if (recordingDuration < 1000) {
                console.log('Recording too short, not sending');
                updateStatus('Recording too short - speak longer', 'error');
                isRecordingAudio = false;
                isCurrentlyRecording = false;
                isListening = false;
                
                // Clear timeouts
                if (silenceTimeout) {
                  clearTimeout(silenceTimeout);
                  silenceTimeout = null;
                }
                if (maxRecordingTimeout) {
                  clearTimeout(maxRecordingTimeout);
                  maxRecordingTimeout = null;
                }
                
                // Stop MediaRecorder
                if (mediaRecorder.state === 'recording') {
                  mediaRecorder.stop();
                }
                return;
              }
              
              isRecordingAudio = false;
              isCurrentlyRecording = false;
              isListening = false;
              
              // Clear timeouts
              if (silenceTimeout) {
                clearTimeout(silenceTimeout);
                silenceTimeout = null;
              }
              if (maxRecordingTimeout) {
                clearTimeout(maxRecordingTimeout);
                maxRecordingTimeout = null;
              }
              
              // Stop MediaRecorder
              if (mediaRecorder.state === 'recording') {
                mediaRecorder.stop();
              }
              
              updateStatus('Processing audio...', 'connecting');
            }
          }
          
          const checkVoiceActivity = () => {
            if (!isListening) return; // Only check when listening
            
            analyser.getByteFrequencyData(dataArray);
            
            // Calculate RMS (Root Mean Square) for better voice detection
            let sum = 0;
            for (let i = 0; i < dataArray.length; i++) {
              sum += dataArray[i] * dataArray[i];
            }
            const rms = Math.sqrt(sum / dataArray.length);
            
            // Simplified thresholds like the original system
            const voiceThreshold = 20; // Moderate threshold
            const silenceThreshold = 10; // Moderate silence threshold
            
            console.log('Voice level:', rms.toFixed(2), 'Threshold:', voiceThreshold, 'Recording:', isCurrentlyRecording);
            
            if (rms > voiceThreshold && isCurrentlyRecording) {
              // Voice detected, update status
              updateStatus('üé§ Recording...', 'connecting');
              
              // Clear any existing timeout
              if (silenceTimeout) {
                clearTimeout(silenceTimeout);
                silenceTimeout = null;
              }
            } else if (rms <= silenceThreshold && isCurrentlyRecording) {
              // Silence detected, stop recording after 3 seconds
              if (!silenceTimeout) {
                silenceTimeout = setTimeout(() => {
                  if (isCurrentlyRecording && isListening) {
                    const recordingDuration = Date.now() - speechStartTime;
                    console.log('Stopping recording after 3 seconds of silence, total duration:', recordingDuration, 'ms');
                    stopRecordingNow();
                  }
                }, 3000); // 3 seconds of silence before stopping
              }
            }
            
            if (isListening) {
              requestAnimationFrame(checkVoiceActivity);
            }
          };
          
          updateStatus('üé§ Audio capture active - speak naturally!', 'connected');
          
        } catch (error) {
          console.error('Error starting audio capture:', error);
          updateStatus('Microphone access denied - please allow microphone access', 'error');
        }
      }

      // Convert Float32Array to PCM16
      function convertFloat32ToPCM16(float32Array) {
        const buffer = new ArrayBuffer(float32Array.length * 2);
        const view = new DataView(buffer);
        let offset = 0;
        
        for (let i = 0; i < float32Array.length; i++, offset += 2) {
          let s = Math.max(-1, Math.min(1, float32Array[i]));
          view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
        }
        
        return new Uint8Array(buffer);
      }

      // Convert PCM16 to base64 for Realtime API
      function pcm16ToBase64(pcm16Array) {
        const uint8Array = new Uint8Array(pcm16Array.buffer);
        let binary = '';
        for (let i = 0; i < uint8Array.length; i++) {
          binary += String.fromCharCode(uint8Array[i]);
        }
        return btoa(binary);
      }

      // Audio variables for buffering
      let audioSource = null;
      let audioChunks = [];
      let isCollectingAudio = false;
      let audioPlaybackTimeout = null;
      let isPlayingAudio = false;

      // Initialize audio context for streaming
      function initAudioContext() {
        if (!audioContext) {
          audioContext = new (window.AudioContext || window.webkitAudioContext)({
            sampleRate: 16000
          });
        }
        return audioContext;
      }

      // Buffer audio chunk for smooth playback
      function bufferAudioChunk(audioData) {
        try {
          console.log('Buffering audio chunk, size:', audioData.length || 'unknown');
          
          // Don't buffer if already playing audio
          if (isPlayingAudio) {
            console.log('Already playing audio, skipping chunk');
            return;
          }
          
          // Start collecting audio if this is the first chunk
          if (!isCollectingAudio) {
            isCollectingAudio = true;
            audioChunks = [];
            console.log('Started collecting audio chunks');
          }
          
          // Add chunk to buffer
          audioChunks.push(audioData);
          
          // Clear any existing timeout
          if (audioPlaybackTimeout) {
            clearTimeout(audioPlaybackTimeout);
          }
          
          // Set timeout to play audio if no more chunks arrive
          audioPlaybackTimeout = setTimeout(() => {
            if (audioChunks.length > 0 && !isPlayingAudio) {
              console.log('Timeout reached, playing buffered audio');
              playBufferedAudio();
            }
          }, 500); // Wait 0.5 seconds for more chunks (faster response)
          
        } catch (error) {
          console.error('Error buffering audio chunk:', error);
        }
      }

      // Play all buffered audio chunks as one smooth stream
      function playBufferedAudio() {
        try {
          if (audioChunks.length === 0) {
            console.log('No audio chunks to play');
            return;
          }
          
          // Prevent multiple simultaneous audio streams
          if (isPlayingAudio) {
            console.log('Already playing audio, skipping playback');
            return;
          }
          
          console.log('Playing buffered audio, chunks:', audioChunks.length);
          isPlayingAudio = true;
          
          // Combine all audio chunks
          let combinedAudioData = '';
          for (const chunk of audioChunks) {
            combinedAudioData += chunk;
          }
          
          // Convert combined base64 to bytes
          const binaryString = atob(combinedAudioData);
          const audioBytes = new Uint8Array(binaryString.length);
          for (let i = 0; i < binaryString.length; i++) {
            audioBytes[i] = binaryString.charCodeAt(i);
          }
          
          // Increase pitch/frequency by modifying the sample rate
          const pitchMultiplier = 1.2; // Increase pitch by 20% (higher frequency)
          const modifiedAudioBytes = increasePitch(audioBytes, pitchMultiplier);
          
          // Create WAV header for combined PCM16 audio
          const sampleRate = 16000;
          const numChannels = 1;
          const bitsPerSample = 16;
          const dataLength = modifiedAudioBytes.length;
          const fileLength = 36 + dataLength;
          
          const wavHeader = new ArrayBuffer(44);
          const view = new DataView(wavHeader);
          
          // WAV header
          const writeString = (offset, string) => {
            for (let i = 0; i < string.length; i++) {
              view.setUint8(offset + i, string.charCodeAt(i));
            }
          };
          
          writeString(0, 'RIFF');
          view.setUint32(4, fileLength, true);
          writeString(8, 'WAVE');
          writeString(12, 'fmt ');
          view.setUint32(16, 16, true);
          view.setUint16(20, 1, true);
          view.setUint16(22, numChannels, true);
          view.setUint32(24, sampleRate, true);
          view.setUint32(28, sampleRate * numChannels * bitsPerSample / 8, true);
          view.setUint16(32, numChannels * bitsPerSample / 8, true);
          view.setUint16(34, bitsPerSample, true);
          writeString(36, 'data');
          view.setUint32(40, dataLength, true);
          
          // Combine header and audio data
          const wavBlob = new Blob([wavHeader, modifiedAudioBytes], { type: 'audio/wav' });
          const audioUrl = URL.createObjectURL(wavBlob);
          
          // Play the combined audio
          const audio = new Audio(audioUrl);
          audio.play().catch(error => {
            console.error('Error playing buffered audio:', error);
            isPlayingAudio = false;
          });
          
          // Clean up URL after playing
          audio.onended = () => {
            URL.revokeObjectURL(audioUrl);
            isPlayingAudio = false;
            console.log('Audio playback completed');
          };
          
          // Reset audio collection state
          audioChunks = [];
          isCollectingAudio = false;
          if (audioPlaybackTimeout) {
            clearTimeout(audioPlaybackTimeout);
            audioPlaybackTimeout = null;
          }
          
        } catch (error) {
          console.error('Error playing buffered audio:', error);
        }
      }

      // Increase pitch/frequency of audio data
      function increasePitch(audioBytes, pitchMultiplier) {
        try {
          // Convert PCM16 bytes to Float32Array
          const samples = new Float32Array(audioBytes.length / 2);
          const dataView = new DataView(audioBytes.buffer);
          
          for (let i = 0; i < samples.length; i++) {
            const sample = dataView.getInt16(i * 2, true);
            samples[i] = sample / 32768.0; // Convert to -1 to 1 range
          }
          
          // Apply pitch shifting by resampling
          const newLength = Math.floor(samples.length / pitchMultiplier);
          const pitchShiftedSamples = new Float32Array(newLength);
          
          for (let i = 0; i < newLength; i++) {
            const sourceIndex = i * pitchMultiplier;
            const index = Math.floor(sourceIndex);
            const fraction = sourceIndex - index;
            
            if (index + 1 < samples.length) {
              // Linear interpolation
              pitchShiftedSamples[i] = samples[index] * (1 - fraction) + samples[index + 1] * fraction;
            } else {
              pitchShiftedSamples[i] = samples[index];
            }
          }
          
          // Convert back to PCM16 bytes
          const result = new Uint8Array(pitchShiftedSamples.length * 2);
          const resultView = new DataView(result.buffer);
          
          for (let i = 0; i < pitchShiftedSamples.length; i++) {
            const sample = Math.max(-1, Math.min(1, pitchShiftedSamples[i]));
            const pcmSample = Math.round(sample * 32767);
            resultView.setInt16(i * 2, pcmSample, true);
          }
          
          return result;
        } catch (error) {
          console.error('Error increasing pitch:', error);
          return audioBytes; // Return original if pitch shifting fails
        }
      }

      // Simple audio completion handler
      function clearAudioQueue() {
        // No longer needed with simple WAV approach
        isSpeaking = false;
      }

      // Add message to conversation
      function addMessage(role, content, isStreaming = false) {
        console.log(`Adding message - Role: ${role}, Content: "${content}", Streaming: ${isStreaming}`);
        
        const conversation = document.getElementById('conversation');
        let messageDiv = conversation.querySelector(`.message.${role}:last-child`);
        
        // For assistant streaming, reuse the last assistant message if it exists and is streaming
        if (role === 'assistant' && isStreaming && messageDiv) {
          console.log(`Reusing existing assistant message for streaming`);
        } else if (!messageDiv || !isStreaming) {
          console.log(`Creating new message div for role: ${role}`);
          messageDiv = document.createElement('div');
          messageDiv.className = `message ${role}`;
          messageDiv.innerHTML = `
            <div class="message-label">${role === 'user' ? 'You' : 'Assistant'}</div>
            <div class="message-content"></div>
          `;
          conversation.appendChild(messageDiv);
        } else {
          console.log(`Using existing message div for role: ${role}`);
        }
        
        const contentDiv = messageDiv.querySelector('.message-content');
        if (isStreaming) {
          contentDiv.textContent += content;
          console.log(`Streaming content to ${role} message: "${contentDiv.textContent}"`);
        } else {
          contentDiv.textContent = content;
          console.log(`Set content for ${role} message: "${content}"`);
        }
        
        conversation.scrollTop = conversation.scrollHeight;
      }

      // Update status
      function updateStatus(message, type = 'default') {
        const statusEl = document.getElementById('status');
        const statusContainer = document.getElementById('statusContainer');
        
        statusEl.textContent = message;
        statusContainer.className = 'status-container';
        
        if (type === 'connected') {
          statusContainer.classList.add('connected');
        } else if (type === 'error') {
          statusContainer.classList.add('error');
        } else if (type === 'connecting') {
          statusContainer.classList.add('connecting');
        }
      }

      // Update latency indicator
      function updateLatency() {
        if (startTime) {
          const latency = Date.now() - startTime;
          latencyHistory.push(latency);
          
          // Keep only last 10 measurements
          if (latencyHistory.length > 10) {
            latencyHistory.shift();
          }
          
          const avgLatency = latencyHistory.reduce((a, b) => a + b, 0) / latencyHistory.length;
          const latencyEl = document.getElementById('latencyIndicator');
          
          latencyEl.textContent = `Latency: ${Math.round(avgLatency)}ms`;
          
          if (avgLatency < 500) {
            latencyEl.className = 'latency-indicator low';
          } else if (avgLatency < 1000) {
            latencyEl.className = 'latency-indicator medium';
          } else {
            latencyEl.className = 'latency-indicator high';
          }
        }
      }

      // Stop recording
      function stopRecording() {
        if (window.currentMediaRecorder && window.currentMediaRecorder.state === 'recording') {
          window.currentMediaRecorder.stop();
          window.currentMediaRecorder = null;
        }
        if (audioContext) {
          audioContext.close();
          audioContext = null;
        }
        isRecording = false;
        isSpeaking = false;
        // Clear audio queue
        clearAudioQueue();
      }

      // Event listeners
      document.getElementById('startSession').addEventListener('click', () => {
        connectWebSocket();
      });

      document.getElementById('stopSession').addEventListener('click', () => {
        if (ws) {
          ws.send(JSON.stringify({ type: 'end_session' }));
          ws.close();
        }
        
        document.getElementById('startSession').style.display = 'block';
        document.getElementById('stopSession').style.display = 'none';
        document.getElementById('micButton').style.display = 'none';
        document.getElementById('conversation').style.display = 'none';
        
        updateStatus('Session stopped', 'default');
        isConnected = false;
        stopRecording();
      });



      // Voice selection handling
      document.querySelectorAll('.voice-option').forEach(option => {
        option.addEventListener('click', () => {
          document.querySelectorAll('.voice-option').forEach(opt => opt.classList.remove('selected'));
          option.classList.add('selected');
          selectedVoice = option.dataset.voice;
          console.log('Selected voice:', selectedVoice);
        });
      });

      // Initialize
      updateStatus('Click "Start Realtime Session" to begin', 'default');
    </script>
  </body>
</html>
